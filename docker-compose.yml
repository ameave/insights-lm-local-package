name: insightslm

include:
  - path:
    - "./local-ai-packaged/docker-compose.yml"
    - "./local-ai-packaged/docker-compose.override.private.yml"
    project_directory: ./local-ai-packaged

volumes:
  whisper_cache:

# Command to import InsightsLM database into supabase-db
# x-insights-import:
#   image: supabase/postgres:15.2.0
#   environment:
#     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
#     POSTGRES_USER: supabase_admin
#     POSTGRES_DB: _supabase
#   entrypoint: /bin/sh
#   command:
#     - psql -U supabase_admin -d _supabase -f /insightslm-migration.sql

services:

  insightslm:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        VITE_SUPABASE_URL: ${API_EXTERNAL_URL}
        VITE_SUPABASE_ANON_KEY: ${ANON_KEY}
    container_name: insightslm
    ports:
      - 3010:80
    restart: unless-stopped

  coqui-tts:
    image: ghcr.io/coqui-ai/tts
    container_name: coqui-tts
    restart: unless-stopped
    ports:
      - 5002
    entrypoint: ["python3", "TTS/server/server.py"]
    command: ["--model_name", "tts_models/en/vctk/vits", "--use_cuda", "true"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  whisper-asr:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    container_name: whisper-asr
    restart: unless-stopped
    ports:
      - "9002:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}  # Options: tiny, base, small, medium, large
      - ASR_ENGINE=${WHISPER_ENGINE:-openai_whisper}  # Options: openai_whisper, faster_whisper
      - ASR_MODEL_PATH=/data/whisper
    volumes:
      - whisper_cache:/data/whisper  # Persist model cache to avoid redownloading
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["gpu-nvidia"]  # Only run with GPU profile
