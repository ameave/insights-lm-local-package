name: insightslm

include:
  - path:
    - "./local-ai-packaged/docker-compose.yml"
    project_directory: ./local-ai-packaged
  - path:
    - "./supabase-insights-lm/docker-compose.yml"
    project_directory: ./supabase-insights-lm


volumes:
  whisper_cache:

x-coqui-tts: &coqui-config
  image: ghcr.io/coqui-ai/tts
  restart: unless-stopped
  environment:
    AUDIO_GENERATION_MODEL: ${AUDIO_GENERATION_MODEL}
  ports:
    - 5002
  entrypoint: ["python3", "TTS/server/server.py"]
  command: ["--model_name", "${AUDIO_GENERATION_MODEL}", "--use_cuda", "true"]

services:

  insightslm:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        VITE_SUPABASE_URL: ${API_EXTERNAL_URL}
        VITE_SUPABASE_ANON_KEY: ${ANON_KEY}
    container_name: insightslm
    ports:
      - 3010:80
    restart: unless-stopped

  coqui-tts:
    container_name: coqui-tts
    <<: *coqui-config
    profiles: ["gpu-nvidia", "gpu-amd"]  # Only run with GPU profiles
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  coqui-tts-cpu:
    container_name: coqui-tts-cpu
    <<: *coqui-config
    profiles: ["cpu", "none"]  # Only run with CPU only profiles
    command: ["--model_name", "${AUDIO_GENERATION_MODEL}", "--use_cuda", "false"]


  whisper-asr:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    container_name: whisper-asr
    profiles: ["gpu-nvidia", "gpu-amd"]  # Only run with GPU profiles
    restart: unless-stopped
    ports:
      - "9002:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}  # Options: tiny, base, small, medium, large
      - ASR_ENGINE=${WHISPER_ENGINE:-openai_whisper}  # Options: openai_whisper, faster_whisper
      - ASR_MODEL_PATH=/data/whisper
    volumes:
      - whisper_cache:/data/whisper  # Persist model cache to avoid redownloading
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
